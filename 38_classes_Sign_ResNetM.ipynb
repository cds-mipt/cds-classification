{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "import keras\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing import image as im\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Input, AveragePooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train phase\n",
    "\n",
    "tf.keras.backend.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose GPU\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Path to train and test datasets\n",
    "train_dir = \"/home/linnik/Datasets/Signs/38_Classes/train/\"   \n",
    "test_dir = \"/home/linnik/Datasets/Signs/38_Classes/test/\"\n",
    "\n",
    "\n",
    "# 2) Target width and height of input images, number of classes, number of train and test images, batch size, epochs\n",
    "img_height, img_width = 72, 72\n",
    "n_classes = len(os.listdir(train_dir))\n",
    "nb_train_samples = sum([len(os.listdir(train_dir + folder)) for folder in os.listdir(train_dir)])\n",
    "nb_test_samples = sum([len(os.listdir(test_dir + folder)) for folder in os.listdir(test_dir)])\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "# 3) Name of file where will be saved trained model\n",
    "model_name = \"/home/linnik/Nets/Sign_ResNetM_72X72x3_38.hdf5\"\n",
    "\n",
    "\n",
    "# 4) Compile parametrs\n",
    "loss_ = 'categorical_crossentropy'\n",
    "optimizer_ = 'adam'\n",
    "metrics_ = ['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255,\n",
    "    zoom_range = 0.2,\n",
    "    rotation_range = 5,\n",
    "    width_shift_range = img_width // 3,\n",
    "    height_shift_range = img_height // 3,\n",
    "    horizontal_flip = False,\n",
    "    vertical_flip = False)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122294 images belonging to 38 classes.\n",
      "Found 16984 images belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00': 0,\n",
       " '01': 1,\n",
       " '02': 2,\n",
       " '03': 3,\n",
       " '04': 4,\n",
       " '05': 5,\n",
       " '06': 6,\n",
       " '07': 7,\n",
       " '08': 8,\n",
       " '09': 9,\n",
       " '10': 10,\n",
       " '11': 11,\n",
       " '12': 12,\n",
       " '13': 13,\n",
       " '14': 14,\n",
       " '15': 15,\n",
       " '16': 16,\n",
       " '17': 17,\n",
       " '18': 18,\n",
       " '19': 19,\n",
       " '20': 20,\n",
       " '21': 21,\n",
       " '22': 22,\n",
       " '23': 23,\n",
       " '24': 24,\n",
       " '25': 25,\n",
       " '26': 26,\n",
       " '27': 27,\n",
       " '28': 28,\n",
       " '29': 29,\n",
       " '30': 30,\n",
       " '31': 31,\n",
       " '32': 32,\n",
       " '33': 33,\n",
       " '34': 34,\n",
       " '35': 35,\n",
       " '36': 36,\n",
       " '37': 37}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(72, 72, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check size\n",
    "\n",
    "for x, y in train_generator:\n",
    "    plt.imshow(x[0])\n",
    "    print(y[0])\n",
    "    print(x[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n",
    "    And the shortcut should have strides=(2,2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides,\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "               name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
    "                      name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def resNetM(input_shape, classes):\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(img_input)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = AveragePooling2D((3, 3), name='avg_pool')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    #x = Dense(512, activation='relu', name='fc01')(x)\n",
    "    #x = Dense(200, activation='relu', name='fc02')(x)\n",
    "    #x = Dropout(0.3, name='dr01')(x)\n",
    "    x = Dense(classes, activation='softmax', name='fc5')(x)\n",
    "\n",
    "    inputs = img_input\n",
    "\n",
    "    model = Model(inputs, x, name='resnetM')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and compile your own model\n",
    "\n",
    "model = resNetM((img_height, img_width, 3), n_classes)\n",
    "model.compile(loss = loss_, optimizer = optimizer_, metrics = metrics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', patience = 8, verbose = 1, min_delta = 1e-4)\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 4, verbose = 1, min_delta = 1e-4)\n",
    "check = ModelCheckpoint(filepath = model_name, monitor = \"val_acc\", save_best_only = True)\n",
    "\n",
    "callbacks_list = [early_stop, reduce_lr, check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "155/955 [===>..........................] - ETA: 4:06 - loss: 3.0617 - acc: 0.3196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955/955 [==============================] - 248s 259ms/step - loss: 1.6685 - acc: 0.5394 - val_loss: 0.5843 - val_acc: 0.8135\n",
      "Epoch 2/100\n",
      "955/955 [==============================] - 177s 186ms/step - loss: 0.3355 - acc: 0.8922 - val_loss: 0.1772 - val_acc: 0.9464\n",
      "Epoch 3/100\n",
      "955/955 [==============================] - 177s 185ms/step - loss: 0.1609 - acc: 0.9512 - val_loss: 0.1683 - val_acc: 0.9536\n",
      "Epoch 4/100\n",
      "955/955 [==============================] - 176s 185ms/step - loss: 0.1256 - acc: 0.9624 - val_loss: 0.1184 - val_acc: 0.9692\n",
      "Epoch 5/100\n",
      "955/955 [==============================] - 176s 184ms/step - loss: 0.1041 - acc: 0.9687 - val_loss: 0.1543 - val_acc: 0.9604\n",
      "Epoch 6/100\n",
      "955/955 [==============================] - 175s 183ms/step - loss: 0.0954 - acc: 0.9718 - val_loss: 0.1694 - val_acc: 0.9513\n",
      "Epoch 7/100\n",
      "955/955 [==============================] - 175s 184ms/step - loss: 0.0855 - acc: 0.9740 - val_loss: 0.0734 - val_acc: 0.9782\n",
      "Epoch 8/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0754 - acc: 0.9776 - val_loss: 0.1035 - val_acc: 0.9702\n",
      "Epoch 9/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0714 - acc: 0.9781 - val_loss: 0.1582 - val_acc: 0.9655\n",
      "Epoch 10/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0683 - acc: 0.9792 - val_loss: 0.0399 - val_acc: 0.9889\n",
      "Epoch 11/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0582 - acc: 0.9822 - val_loss: 0.0815 - val_acc: 0.9756\n",
      "Epoch 12/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0556 - acc: 0.9827 - val_loss: 0.0870 - val_acc: 0.9762\n",
      "Epoch 13/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0532 - acc: 0.9841 - val_loss: 0.0428 - val_acc: 0.9892\n",
      "Epoch 14/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0506 - acc: 0.9844 - val_loss: 0.0474 - val_acc: 0.9876\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0318 - acc: 0.9899 - val_loss: 0.0201 - val_acc: 0.9947\n",
      "Epoch 16/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0211 - val_acc: 0.9947\n",
      "Epoch 17/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0220 - acc: 0.9931 - val_loss: 0.0199 - val_acc: 0.9958\n",
      "Epoch 18/100\n",
      "955/955 [==============================] - 172s 181ms/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0221 - val_acc: 0.9951\n",
      "Epoch 19/100\n",
      "955/955 [==============================] - 173s 181ms/step - loss: 0.0199 - acc: 0.9935 - val_loss: 0.0172 - val_acc: 0.9958\n",
      "Epoch 20/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0176 - val_acc: 0.9959\n",
      "Epoch 21/100\n",
      "955/955 [==============================] - 172s 181ms/step - loss: 0.0184 - acc: 0.9943 - val_loss: 0.0193 - val_acc: 0.9954\n",
      "Epoch 22/100\n",
      "955/955 [==============================] - 173s 181ms/step - loss: 0.0171 - acc: 0.9946 - val_loss: 0.0202 - val_acc: 0.9953\n",
      "Epoch 23/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0152 - acc: 0.9949 - val_loss: 0.0179 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 24/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0173 - val_acc: 0.9963\n",
      "Epoch 25/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.0174 - val_acc: 0.9956\n",
      "Epoch 26/100\n",
      "955/955 [==============================] - 175s 183ms/step - loss: 0.0130 - acc: 0.9957 - val_loss: 0.0181 - val_acc: 0.9963\n",
      "Epoch 27/100\n",
      "955/955 [==============================] - 175s 183ms/step - loss: 0.0140 - acc: 0.9955 - val_loss: 0.0166 - val_acc: 0.9958\n",
      "Epoch 28/100\n",
      "955/955 [==============================] - 175s 183ms/step - loss: 0.0138 - acc: 0.9955 - val_loss: 0.0182 - val_acc: 0.9957\n",
      "Epoch 29/100\n",
      "955/955 [==============================] - 175s 184ms/step - loss: 0.0135 - acc: 0.9955 - val_loss: 0.0210 - val_acc: 0.9956\n",
      "Epoch 30/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0170 - val_acc: 0.9961\n",
      "Epoch 31/100\n",
      "955/955 [==============================] - 173s 181ms/step - loss: 0.0126 - acc: 0.9960 - val_loss: 0.0180 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 32/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0135 - acc: 0.9955 - val_loss: 0.0195 - val_acc: 0.9957\n",
      "Epoch 33/100\n",
      "955/955 [==============================] - 175s 183ms/step - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0153 - val_acc: 0.9960\n",
      "Epoch 34/100\n",
      "955/955 [==============================] - 176s 184ms/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.0206 - val_acc: 0.9959\n",
      "Epoch 35/100\n",
      "955/955 [==============================] - 175s 183ms/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0170 - val_acc: 0.9957\n",
      "Epoch 36/100\n",
      "955/955 [==============================] - 174s 183ms/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.0214 - val_acc: 0.9953\n",
      "Epoch 37/100\n",
      "955/955 [==============================] - 175s 183ms/step - loss: 0.0135 - acc: 0.9954 - val_loss: 0.0147 - val_acc: 0.9963\n",
      "Epoch 38/100\n",
      "955/955 [==============================] - 174s 183ms/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.0204 - val_acc: 0.9958\n",
      "Epoch 39/100\n",
      "955/955 [==============================] - 174s 183ms/step - loss: 0.0125 - acc: 0.9960 - val_loss: 0.0187 - val_acc: 0.9958\n",
      "Epoch 40/100\n",
      "955/955 [==============================] - 175s 183ms/step - loss: 0.0128 - acc: 0.9958 - val_loss: 0.0201 - val_acc: 0.9955\n",
      "Epoch 41/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0128 - acc: 0.9958 - val_loss: 0.0153 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 42/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0130 - acc: 0.9959 - val_loss: 0.0191 - val_acc: 0.9957\n",
      "Epoch 43/100\n",
      "955/955 [==============================] - 174s 183ms/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.0173 - val_acc: 0.9961\n",
      "Epoch 44/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0200 - val_acc: 0.9955\n",
      "Epoch 45/100\n",
      "955/955 [==============================] - 174s 182ms/step - loss: 0.0127 - acc: 0.9960 - val_loss: 0.0170 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 00045: early stopping\n",
      "CPU times: user 2h 54min 31s, sys: 13min 43s, total: 3h 8min 14s\n",
      "Wall time: 2h 12min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train model\n",
    "\n",
    "model_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs = epochs,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = nb_test_samples // batch_size,\n",
    "    callbacks = callbacks_list,\n",
    "    steps_per_epoch = nb_train_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test phase\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "\n",
    "loaded_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16984 images belonging to 38 classes.\n",
      "Average time to pridict one photo using generator is 0.010195410888935113\n"
     ]
    }
   ],
   "source": [
    "# Speed test using generator\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = 1,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "nb_samples = len(test_generator.filenames)\n",
    "\n",
    "start_time = time.time()\n",
    "predict = loaded_model.predict_generator(test_generator, steps = nb_samples)\n",
    "print(\"Average time to pridict one photo using generator is\", (time.time() - start_time)/nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision_Recall_metrics(test_dir, model):\n",
    "    n_classes = len(os.listdir(test_dir))\n",
    "    \n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    times = []\n",
    "\n",
    "    TP = [0 for i in range(n_classes)]\n",
    "    FN = [0 for i in range(n_classes)]\n",
    "    FP = [0 for i in range(n_classes)]\n",
    "    precision = [0 for i in range(n_classes)]\n",
    "    recall = [0 for i in range(n_classes)]\n",
    "\n",
    "    for folder in os.listdir(test_dir):\n",
    "        for img in os.listdir(test_dir + \"/\" + folder):\n",
    "            imag = im.load_img(test_dir + \"/\" + folder + \"/\" + img, target_size = (img_height, img_width))\n",
    "\n",
    "            photo_ar = im.img_to_array(imag)\n",
    "            photo_ar = np.expand_dims(photo_ar, axis = 0)\n",
    "            photo_ar /= 255\n",
    "\n",
    "            start_time = time.time()\n",
    "            ans = np.argmax(model.predict(photo_ar))\n",
    "            times.append(time.time() - start_time)\n",
    "\n",
    "            if ans == int(folder):\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "\n",
    "\n",
    "            if ans == int(folder):\n",
    "                TP[ans] += 1\n",
    "            if ans != int(folder):\n",
    "                FN[ans] += 1\n",
    "                FP[int(folder)] += 1\n",
    "\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        try:\n",
    "            precision[i] = TP[i]/(TP[i] + FP[i])\n",
    "        except:\n",
    "            precision[i] = 0\n",
    "\n",
    "        try:\n",
    "            recall[i] = TP[i]/(TP[i] + FN[i])\n",
    "        except:\n",
    "            recall[i] = 0\n",
    "    precision = np.array(precision)\n",
    "    recall = np.array(recall)\n",
    "    \n",
    "    accuracy = right/(right + wrong)\n",
    "    avr_time = np.array(times).mean()\n",
    "    \n",
    "    return accuracy, avr_time, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.9958195949128592\n",
      "Average time to predict one photo without generator is 0.00989689975204863 seconds\n",
      "\n",
      "Precision: [1.         0.99649123 0.99560761 1.         1.         0.98214286\n",
      " 0.99122807 0.99375    0.99685535 0.99865093 0.99437095 0.9935\n",
      " 1.         0.99590164 1.         0.98198198 1.         0.99516908\n",
      " 0.99775785 0.99713467 0.99758454 0.99726027 0.99016393 1.\n",
      " 1.         0.99280576 1.         1.         1.         1.\n",
      " 1.         1.         1.         0.98706897 1.         0.97826087\n",
      " 0.99264706 0.97794118]\n",
      "Recall: [0.99579832 1.         0.99415205 1.         0.99672131 0.98802395\n",
      " 0.99122807 0.98757764 0.98447205 0.99596367 0.9946509  0.99899447\n",
      " 1.         1.         0.9921875  1.         0.9973545  0.99516908\n",
      " 1.         0.99145299 1.         0.99453552 1.         0.99319728\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.99134199 0.9787234  1.\n",
      " 0.97826087 0.97080292]\n",
      "\n",
      "Precision max, min, mean: 1.0 0.9779411764705882 0.9953756524496713\n",
      "Recall max, min, mean: 1.0 0.9708029197080292 0.9950160128073954\n",
      "CPU times: user 1min 46s, sys: 17.5 s, total: 2min 3s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "accuracy, avr_time, precision, recall = Precision_Recall_metrics(test_dir, loaded_model)\n",
    "            \n",
    "\n",
    "print(\"Accuracy on test set is\", accuracy)\n",
    "print(\"Average time to predict one photo without generator is\", avr_time, \"seconds\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Precision max, min, mean:\", precision.max(), precision.min(), precision.mean())\n",
    "print(\"Recall max, min, mean:\", recall.max(), recall.min(), recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAF1CAYAAADiNYyJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAY/0lEQVR4nO3df7RlZX3f8c83A4oIEZmZZFkGHGxIItIJkzVqWHFVF8EIGhldDQZqKHSZkKxkXLSYNhgSpDa2iT+S1IYYMbGINhCapHZIaCE1tqY1IGMcNUAJE0SZCQKCoDQioN/+cc/Y63V+HOae653xeb3WmsXZ++yzzzOPe7zv2bPP2dXdAQCA0Xzbcg8AAACWgxAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYYAZqao7q+pLVfVwVX22qi6vqsOWeTynzFteW1VdVR9bsN2qqnq0qu78pg8SYBkJYYDZenl3H5bkxCTrk7x+mcezK4dW1Qnzlv9xkk8t12AAlosQBlgC3f3ZJNdlLohTVU+uqrdW1Weq6p6q+u2qesrkuVVV9cdV9WBVPVBVf15V3zZ57s6q+rmq+kRVPVRVv19Vh+x8n6r6karaOnnth6tq3WT9e5Mck+SayRnqfzlveO9Ncs685X+S5IqlnA+A/ZEQBlgCVbUmyWlJtk1W/UqS785cGH9XkqOSXDx57nVJtidZneQ7k/xCkp63u1clOTXJsUnWJTl38h7rk7w7yU8lWZnknUk2V9WTu/vsJJ/J5Ax1d7953v7el+TMqlpRVccnOSzJjTP7zQMcIIQwwGy9v6q+mOSuJPcmeUNVVZLzkvzz7n6gu7+Y5N8kOXPymseSPCPJM7v7se7+8+6eH8Jv7+6/7e4HklyTyVnmyT7f2d03dvdXuvs9Sb6c5Af2MsbtSW5Lckrmzga/d7G/aYADkRAGmK1XdPfhSV6U5HuTrMrcmd5Dk3x0cgnDg0n+22R9krwlc2eOr6+qO6rqwgX7/Oy8x3+XuTO4SfLMJK/buc/Jfo9O8vemGOcVmTuzfFaEMDAoIQywBLr7fya5PMlbk3wuyZeSPKe7j5j8etrkQ3Xp7i929+u6+1lJTk9yQVX90BRvc1eSN83b5xHdfWh3X7lzGHt47R8meVmSO7r7M/v2uwQ4sAlhgKXzG0lenOQfJHlXkl+vqu9Ikqo6qqpeMnn8I1X1XZNLKB5K8pUkX51i/+9K8tNV9fya89SqellVHT55/p4kz9rVC7v7/yY5OclPLOL3B3BAE8IAS6S778vcJQgXJ/n5zF3+cENVfSHJf0/yPZNNj5ssP5zkL5L8Vnd/cIr9b0nyk0l+M8nnJ/s/d94m/zbJL04um/i5Xb2+u/9m3353AAe++vrPYwAAwBicEQYAYEhCGACAIQlhAACGJIQBABiSEAYAYEgHLdcbr1q1qteuXbtcbw8AwCA++tGPfq67Vy9cv2whvHbt2mzZsmW53h4AgEFU1ad3td6lEQAADEkIAwAwJCEMAMCQlu0aYQAAZuOxxx7L9u3b88gjjyz3UJbVIYcckjVr1uTggw+eanshDABwgNu+fXsOP/zwrF27NlW13MNZFt2d+++/P9u3b8+xxx471WtcGgEAcIB75JFHsnLlymEjOEmqKitXrnxCZ8WFMADAt4CRI3inJzoHQhgAgEVbsWJFTjzxxJxwwgl5+ctfngcffHCm+7/88suzadOmJMkll1ySt771rYvep2uEAQC+xay98E9mur87f+Vle93mKU95SrZu3ZokOeecc3LppZfmoosumuk4Zs0ZYQAAZuqkk07Kjh07vrb8lre8Jc997nOzbt26vOENb/ja+iuuuCLr1q3L933f9+Xss89OklxzzTV5/vOfn/Xr1+eUU07JPffcs2Tj3OsZ4ap6d5IfSXJvd5+wi+cryb9L8tIkf5fk3O7+y1kPFACA/d9XvvKVfOADH8hrXvOaJMn111+f22+/PR/5yEfS3Tn99NPzoQ99KCtXrswv//Iv58Mf/nBWrVqVBx54IEnyghe8IDfccEOqKr/zO7+TN7/5zXnb2962JGOd5tKIy5P8ZpIrdvP8aUmOm/x6fpJ3TP4LAMAgvvSlL+XEE0/Mjh078uxnPzsvfvGLk8yF8PXXX5/169cnSR5++OHcfvvt+fjHP54zzjgjq1atSpIceeSRSea+Cu7HfuzHcvfdd+fRRx+d+qvQ9sVeL43o7g8leWAPm2xMckXPuSHJEVX1jFkNEACA/d/Oa4Q//elPp7tz6aWXJpn7ft/Xv/712bp1a7Zu3Zpt27Z97Wzxrrz2ta/Npk2b8slPfjLvfOc7l/QmIbP4sNxRSe6at7x9su7uhRtW1XlJzkuSY445ZgZvvTye6AXo01xgvpTvvxRjYHkt9hhY7tcv1nK//2LtD+Mf/f/HlvvPwIH++n3Zx2Jfv3Afy/365bZw/O86/Rl5bPtsv6VhXx166KF5+9vfnle84hX5mZ/5mbzkJS/JL/3SL+XVr351DjvssOzYsSMHH3xwTj755Lzyla/MBRdckJUrV+aBBx7IkUcemYceeihHHXVUkuQ973nPko71m/qtEd19WZLLkmTDhg39zXzv+Zb7B8ByO9D/8C8383fg878hwNJav3591q1blyuvvDJnn312br311px00klJksMOOyzve9/78pznPCcXXXRRXvjCF2bFihVZv359Lr/88lxyySU544wz8vSnPz0nn3xyPvWpTy3ZOGcRwjuSHD1vec1kHbvhhzCj/2XsQOfP8PLPwXK/P+zvNm/6wW9Yt27NEUv6ng8//PDXLV9zzTVfe3z++efn/PPP/4bXnHPOOTnnnHO+bt3GjRuzcePGb9j23HPPzbnnnptk7nuEZ2EWIbw5yaaquipzH5J7qLu/4bII2MkPMBwDi2P+YPn5c/itYZqvT7syyYuSrKqq7UnekOTgJOnu305ybea+Om1b5r4+7Z8u1WDZPyz3H/7lfn8A4FvDXkO4u8/ay/Od5GdnNiJYYkIaAEjcWQ4A4IDX6cydmxzbE52Db+q3RgAAMPt/nfz0g49l5cov5KBDvz1zN/3ds0/sw1etLfWH7Raru3P//ffnkEMOmfo1QhgA4AD372/8fF6b5JlHfC6VXYfwrV98ytce3/P5Lz3h95j/+v3VIYcckjVr1ky9vRAGADjAfeHLX82bPnT/HreZf0b5tMFvSLKTa4QBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABjSVCFcVadW1W1Vta2qLtzF88dU1Qer6mNV9YmqeunshwoAALOz1xCuqhVJLk1yWpLjk5xVVccv2OwXk1zd3euTnJnkt2Y9UAAAmKVpzgg/L8m27r6jux9NclWSjQu26STfPnn8tCR/O7shAgDA7E0TwkcluWve8vbJuvkuSfLjVbU9ybVJXrurHVXVeVW1paq23HffffswXAAAmI1ZfVjurCSXd/eaJC9N8t6q+oZ9d/dl3b2huzesXr16Rm8NAABP3DQhvCPJ0fOW10zWzfeaJFcnSXf/RZJDkqyaxQABAGApTBPCNyU5rqqOraonZe7DcJsXbPOZJD+UJFX17MyFsGsfAADYb+01hLv78SSbklyX5NbMfTvEzVX1xqo6fbLZ65L8ZFV9PMmVSc7t7l6qQQMAwGIdNM1G3X1t5j4EN3/dxfMe35LkB2c7NAAAWDruLAcAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCEMAAAQxLCAAAMSQgDADAkIQwAwJCmCuGqOrWqbquqbVV14W62eVVV3VJVN1fV7812mAAAMFsH7W2DqlqR5NIkL06yPclNVbW5u2+Zt81xSV6f5Ae7+/NV9R1LNWAAAJiFac4IPy/Jtu6+o7sfTXJVko0LtvnJJJd29+eTpLvvne0wAQBgtqYJ4aOS3DVveftk3XzfneS7q+p/V9UNVXXqrnZUVedV1Zaq2nLfffft24gBAGAGZvVhuYOSHJfkRUnOSvKuqjpi4UbdfVl3b+juDatXr57RWwMAwBM3TQjvSHL0vOU1k3XzbU+yubsf6+5PJfnrzIUxAADsl6YJ4ZuSHFdVx1bVk5KcmWTzgm3en7mzwamqVZm7VOKOGY4TAABmaq8h3N2PJ9mU5Loktya5urtvrqo3VtXpk82uS3J/Vd2S5INJ/kV3379UgwYAgMXa69enJUl3X5vk2gXrLp73uJNcMPkFAAD7PXeWAwBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSFOFcFWdWlW3VdW2qrpwD9v9o6rqqtowuyECAMDs7TWEq2pFkkuTnJbk+CRnVdXxu9ju8CTnJ7lx1oMEAIBZm+aM8POSbOvuO7r70SRXJdm4i+3+dZJfTfLIDMcHAABLYpoQPirJXfOWt0/WfU1VfX+So7v7T2Y4NgAAWDKL/rBcVX1bkl9L8roptj2vqrZU1Zb77rtvsW8NAAD7bJoQ3pHk6HnLaybrdjo8yQlJ/kdV3ZnkB5Js3tUH5rr7su7e0N0bVq9eve+jBgCARZomhG9KclxVHVtVT0pyZpLNO5/s7oe6e1V3r+3utUluSHJ6d29ZkhEDAMAM7DWEu/vxJJuSXJfk1iRXd/fNVfXGqjp9qQcIAABL4aBpNurua5Ncu2DdxbvZ9kWLHxYAACwtd5YDAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIQhgAgCEJYQAAhiSEAQAYkhAGAGBIU4VwVZ1aVbdV1baqunAXz19QVbdU1Seq6gNV9czZDxUAAGZnryFcVSuSXJrktCTHJzmrqo5fsNnHkmzo7nVJ/iDJm2c9UAAAmKVpzgg/L8m27r6jux9NclWSjfM36O4PdvffTRZvSLJmtsMEAIDZmiaEj0py17zl7ZN1u/OaJP91MYMCAICldtAsd1ZVP55kQ5IX7ub585KclyTHHHPMLN8aAACekGnOCO9IcvS85TWTdV+nqk5JclGS07v7y7vaUXdf1t0bunvD6tWr92W8AAAwE9OE8E1JjquqY6vqSUnOTLJ5/gZVtT7JOzMXwffOfpgAADBbew3h7n48yaYk1yW5NcnV3X1zVb2xqk6fbPaWJIcl+U9VtbWqNu9mdwAAsF+Y6hrh7r42ybUL1l087/EpMx4XAAAsKXeWAwBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSEIYAIAhCWEAAIYkhAEAGJIQBgBgSFOFcFWdWlW3VdW2qrpwF88/uap+f/L8jVW1dtYDBQCAWdprCFfViiSXJjktyfFJzqqq4xds9pokn+/u70ry60l+ddYDBQCAWZrmjPDzkmzr7ju6+9EkVyXZuGCbjUneM3n8B0l+qKpqdsMEAIDZmiaEj0py17zl7ZN1u9ymux9P8lCSlbMYIAAALIXq7j1vUPWjSU7t7p+YLJ+d5PndvWneNn812Wb7ZPlvJtt8bsG+zkty3mTxe5LcNqvfyIysSvK5vW7F7pi/xTOHi2cOF8f8LZ45XBzzt3jm8Bs9s7tXL1x50BQv3JHk6HnLaybrdrXN9qo6KMnTkty/cEfdfVmSy6Yd8TdbVW3p7g3LPY4DlflbPHO4eOZwcczf4pnDxTF/i2cOpzfNpRE3JTmuqo6tqiclOTPJ5gXbbE5yzuTxjyb5s97bqWYAAFhGez0j3N2PV9WmJNclWZHk3d19c1W9McmW7t6c5HeTvLeqtiV5IHOxDAAA+61pLo1Id1+b5NoF6y6e9/iRJGfMdmjLYr+9bOMAYf4WzxwunjlcHPO3eOZwcczf4pnDKe31w3IAAPCtyC2WAQAYkhDO3m8hzd5V1Z1V9cmq2lpVW5Z7PAeCqnp3Vd07+frBneuOrKo/rarbJ/99+nKOcX+2m/m7pKp2TI7DrVX10uUc4/6sqo6uqg9W1S1VdXNVnT9Z7xic0h7m0HE4pao6pKo+UlUfn8zhv5qsP7aqbpz8XP79yYf1WWAP83d5VX1q3jF44nKPdX81/KURk1tI/3WSF2fuZiE3JTmru29Z1oEdYKrqziQbFn53NLtXVf8wycNJrujuEybr3pzkge7+lclfyp7e3T+/nOPcX+1m/i5J8nB3v3U5x3YgqKpnJHlGd/9lVR2e5KNJXpHk3DgGp7KHOXxVHIdTmdyF9qnd/XBVHZzkfyU5P8kFSf6ou6+qqt9O8vHufsdyjnV/tIf5++kkf9zdf7CsAzwAOCM83S2kYea6+0OZ+5aV+ebfrvw9mfuhyi7sZv6YUnff3d1/OXn8xSS3Zu4uoY7BKe1hDplSz3l4snjw5FcnOTnJzohzHO7GHuaPKQnh6W4hzd51kuur6qOTOwiyb76zu++ePP5sku9czsEcoDZV1Scml074Z/0pVNXaJOuT3BjH4D5ZMIeJ43BqVbWiqrYmuTfJnyb5myQPdvfjk038XN6DhfPX3TuPwTdNjsFfr6onL+MQ92tCmFl5QXd/f5LTkvzs5J+tWYTJTWn8zf6JeUeSv5/kxCR3J3nb8g5n/1dVhyX5wyT/rLu/MP85x+B0djGHjsMnoLu/0t0nZu7Otc9L8r3LPKQDysL5q6oTkrw+c/P43CRHJnF5024I4eluIc1edPeOyX/vTfKfM/d/Zjxx90yuO9x5/eG9yzyeA0p33zP5ofDVJO+K43CPJtcU/mGS/9jdfzRZ7Rh8AnY1h47DfdPdDyb5YJKTkhxRVTvvdeDn8hTmzd+pk8t2uru/nOQ/xDG4W0J4ultIswdV9dTJB0VSVU9N8sNJ/mrPr2I35t+u/Jwk/2UZx3LA2RlwE6+M43C3Jh+y+d0kt3b3r817yjE4pd3NoeNwelW1uqqOmDx+SuY+uH5r5oLuRyebOQ53Yzfz93/m/WW2Mnd9tWNwN4b/1ogkmXy1zW/k/99C+k3LPKQDSlU9K3NngZO5uxX+njncu6q6MsmLkqxKck+SNyR5f5KrkxyT5NNJXtXdPhC2C7uZvxdl7p+jO8mdSX5q3vWuzFNVL0jy50k+meSrk9W/kLlrXB2DU9jDHJ4Vx+FUqmpd5j4MtyJzJ+eu7u43Tn6uXJW5f9b/WJIfn5zdZJ49zN+fJVmdpJJsTfLT8z5UxzxCGACAIbk0AgCAIQlhAACGJIQBABiSEAYAYEhCGACAIQlhAACGJIQBABiSEAYAYEj/D7u4jC6TCgQbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(range(n_classes), recall, label = \"Recall\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.title(\"ResnetM\")\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(range(n_classes), precision, label = \"Precision\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.title(\"ResNetM\")\n",
    "\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-d22d3fe33d9a>:3: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.remove_training_nodes`\n",
      "WARNING:tensorflow:From <ipython-input-31-d22d3fe33d9a>:4: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/linnik/.conda/envs/linnik/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 146 variables.\n",
      "INFO:tensorflow:Converted 146 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "def freeze_graph(graph, session, output, save_pb_dir = '.', save_pb_name = 'frozen_model.pb', save_pb_as_text=False):\n",
    "    with graph.as_default():\n",
    "        graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())\n",
    "        graphdef_frozen = tf.graph_util.convert_variables_to_constants(session, graphdef_inf, output)\n",
    "        graph_io.write_graph(graphdef_frozen, save_pb_dir, save_pb_name, as_text=save_pb_as_text)\n",
    "        return graphdef_frozen\n",
    "\n",
    "session = tf.keras.backend.get_session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "\n",
    "input_names = [t.op.name for t in model.inputs]\n",
    "output_names = [t.op.name for t in model.outputs]\n",
    "\n",
    "frozen_graph = freeze_graph(session.graph, session, [out.op.name for out in model.outputs], \n",
    "                            save_pb_dir = '.', save_pb_name = 'frozen_model_ct2_Sign_ResNetM_72X72x3_33.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_1'] ['fc5/Softmax']\n"
     ]
    }
   ],
   "source": [
    "# Prints input and output nodes names, take notes of them\n",
    "\n",
    "print(input_names, output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for photo in os.listdir(test_dir + \"34\"):\n",
    "    imag = im.load_img(test_dir + \"34/\" + photo, target_size = (img_height, img_width))\n",
    "\n",
    "    photo_ar = im.img_to_array(imag)\n",
    "    photo_ar = np.expand_dims(photo_ar, axis = 0)\n",
    "    photo_ar /= 255\n",
    "    \n",
    "    pred = np.argmax(loaded_model.predict(photo_ar))    \n",
    "    if pred != 34:\n",
    "        shutil.move(test_dir + \"34/\" + photo, train_dir + \"34/\" + photo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune NKBVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_dir = \"/home/linnik/Datasets/Signs/nkbvs_ts_crops_RGB/\"\n",
    "temp_dir = \"/home/linnik/Datasets/Signs/temp/\"\n",
    "\n",
    "for img in os.listdir(photos_dir):\n",
    "    imag = im.load_img(photos_dir + img, target_size = (img_height, img_width))\n",
    "\n",
    "    photo_ar = im.img_to_array(imag)\n",
    "    photo_ar = np.expand_dims(photo_ar, axis = 0)\n",
    "    photo_ar /= 255\n",
    "\n",
    "    pred = np.argmax(loaded_model.predict(photo_ar))\n",
    "    \n",
    "    os.makedirs(temp_dir + str(pred), exist_ok = True)\n",
    "    shutil.copy(photos_dir + img, temp_dir + str(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
